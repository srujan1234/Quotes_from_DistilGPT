{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"*****Generating inspirational quotes using a pre-trained GPT-2 model and saving them to a file *******\n\n1. **Check for GPU and Load Model:**\n   - The code checks if a GPU is available, and if so, it sets the device to \"cuda.\" It then loads the GPT-2 tokenizer and model (\"noelmathewisaac/inspirational-quotes-distilgpt2\") on the specified device.\n\n2. **Load Preprocessed Dataset:**\n   - The path to a preprocessed dataset is defined using `dataset_path`. The preprocessed dataset contains text data, including a column with the first 3 words of each entry. It loads this dataset using pandas.\n\n3. **Generated File Path:**\n   - The `generated_file_path` is specified as the path to save the generated quotes. It should be a writable path in Kaggle's working directory.\n\n4. **Generate Quotes for Each Row:**\n   - The function `generate_quotes_for_each_row` is defined to generate quotes for each row in the dataset.\n   - The function checks if there are existing quotes in the generated file (if it exists) to determine the starting index.\n   - It then iterates through the rows, uses the first 3 words of each entry as seed text, and generates quotes using the GPT-2 model.\n   - If quotes are generated in batches of 100 (controlled by `save_every`), they are appended to the existing file to prevent overwriting.\n   - The generated quotes, along with their source row index, are stored in the `generated_quotes` list.\n\n5. **Append Quotes to the Existing File:**\n   - After generating quotes, if there are quotes in the `generated_quotes` list (i.e., after every 100 quotes or at the end), they are appended to the existing file in the working directory using pandas. This is done to save the quotes without overwriting the existing ones.\n\n6. **Generate Quotes for All Rows:**\n   - The code calls the `generate_quotes_for_each_row` function to generate quotes for all rows in the preprocessed dataset.\n\n7. **Copy the Generated File:**\n   - The generated quotes file is copied from its original location to the working directory using the `shutil.copy` function. This ensures that you can easily access the generated quotes within Kaggle's environment.\n\n","metadata":{}},{"cell_type":"code","source":"!pip install transformers\n!pip install --upgrade tensorflow\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"af7rLXY4hBXe","outputId":"8538f0e9-7f19-4558-e1b6-89d194682935","execution":{"iopub.status.busy":"2023-10-26T00:17:04.674005Z","iopub.execute_input":"2023-10-26T00:17:04.674407Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --upgrade tensorflow==2.5.0 tensorflow-probability==0.12.2\n","metadata":{"execution":{"iopub.status.busy":"2023-10-26T00:16:57.543925Z","iopub.execute_input":"2023-10-26T00:16:57.544310Z","iopub.status.idle":"2023-10-26T00:16:59.842669Z","shell.execute_reply.started":"2023-10-26T00:16:57.544282Z","shell.execute_reply":"2023-10-26T00:16:59.841666Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.5.0 (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.15.0rc0)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.5.0\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# Use a pipeline as a high-level helper\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"noelmathewisaac/inspirational-quotes-distilgpt2\")","metadata":{"id":"zhJeWIAog12k","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"noelmathewisaac/inspirational-quotes-distilgpt2\")\nmodel = AutoModelForCausalLM.from_pretrained(\"noelmathewisaac/inspirational-quotes-distilgpt2\")","metadata":{"id":"BJys8gusg12o","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###############IGNORE\n##file_path = '/content/drive/My Drive/prepro/generated_quotes.csv'\n\n# Open the file in write mode to clear its content\nwith open(file_path, 'w') as file:\n    file.truncate(0)\n","metadata":{"id":"kyjnrgWWMwTd"}},{"cell_type":"code","source":"import random\nimport torch\nimport pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport os\n\n# Check if a GPU is available and use it\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"noelmathewisaac/inspirational-quotes-distilgpt2\")\nmodel = AutoModelForCausalLM.from_pretrained(\"noelmathewisaac/inspirational-quotes-distilgpt2\").to(device)\n\n# Define the path to your dataset in Kaggle\ndataset_path = '/kaggle/input/sammyyy/preprocessed_quotes_no_quote_author_category (1).csv'\n\n# Load the preprocessed dataset\npreprocessed_data = pd.read_csv(dataset_path)\n\n# Define the path to save the generated file in Kaggle's working directory\ngenerated_file_path = '/kaggle/working/generated_quotes (1).csv'\n\n# Function to generate quotes for each row without overwriting existing quotes\ndef generate_quotes_for_each_row(dataframe, max_rows, generated_file_path, max_length=100, save_every=100):\n    generated_quotes = []\n\n    if os.path.exists(generated_file_path):\n        # Load existing generated quotes and determine the starting index in the dataset\n        existing_quotes_data = pd.read_csv(generated_file_path, delimiter='\\t')\n        num_existing_quotes = len(existing_quotes_data)\n    else:\n        num_existing_quotes = 0\n\n    for index in range(num_existing_quotes, max_rows):\n        entry = dataframe.iloc[index][\"first_3_words\"]\n        seed_text = entry\n        input_ids = tokenizer.encode(seed_text, return_tensors=\"pt\").to(device)\n        attention_mask = torch.ones(input_ids.shape, device=device)\n        output = model.generate(input_ids, max_length=max_length, no_repeat_ngram_size=20, top_k=50, pad_token_id=model.config.eos_token_id, attention_mask=attention_mask)\n        quote = tokenizer.decode(output[0], skip_special_tokens=True)\n\n        generated_quotes.append({\"Generated_Quote\": quote, \"Source_Row\": f\"Row {index + 1}\"})\n\n        if len(generated_quotes) % save_every == 0:\n            # Append newly generated quotes to the existing file every 100 quotes\n            generated_quotes_data = pd.DataFrame(generated_quotes)\n            generated_quotes_data.to_csv(generated_file_path, mode='a', header=False, index=False, sep='\\t')\n            generated_quotes = []  # Clear the list to avoid saving the same quotes multiple times\n            print(f\"Yay {index + 1} saved\")\n\n    # Append any remaining newly generated quotes\n    if generated_quotes:\n        generated_quotes_data = pd.DataFrame(generated_quotes)\n        generated_quotes_data.to_csv(generated_file_path, mode='a', header=False, index=False, sep='\\t')\n        print(f\"Yay {max_rows} saved\")\n\n# Generate up to 1 quote per row for the maximum number of rows\nmax_rows_to_generate = len(preprocessed_data)\ngenerate_quotes_for_each_row(preprocessed_data, max_rows_to_generate, generated_file_path)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kX_J2fKig12p","outputId":"4bc37f11-4860-43bd-e692-d706353bca69","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}