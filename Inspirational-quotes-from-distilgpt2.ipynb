{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**This code snippet demonstrates how to generate inspirational quotes using a pre-trained GPT-2 model for fine-tuned text generation. The process involves loading a pre-trained model and fine-tuning it for generating inspirational quotes based on seed text. Here's a breakdown of the key steps in this code**\n\n1. **Use a Pre-trained Model with a Pipeline:**\n   - The code first uses the Transformers library to create a text generation pipeline. It specifies the model to be used as \"noelmathewisaac/inspirational-quotes-distilgpt2.\" This pre-trained model is designed for generating inspirational quotes.\n\n2. **Load Model Directly:**\n   - Alternatively, the code provides instructions on how to load the model directly using its tokenizer and model architecture. This approach offers more flexibility for advanced users who may want to customize model parameters.\n\n3. **Check for GPU Availability:**\n   - The code checks for the availability of a GPU. If a GPU is available, it sets the device to \"cuda\" to leverage GPU acceleration; otherwise, it uses the CPU.\n\n4. **Load Tokenizer and Model:**\n   - The tokenizer and model are loaded from the pre-trained \"noelmathewisaac/inspirational-quotes-distilgpt2\" model. The model is loaded on the specified device (GPU or CPU).\n\n5. **Load Preprocessed Dataset:**\n   - The code loads a preprocessed dataset containing text data from a CSV file. The path to the dataset should be provided, and the specific column with text data (in this case, \"first_3_words\") is selected.\n\n6. **Generate Inspirational Quotes:**\n   - The code defines a function, `generate_quotes_for_each_row`, which generates inspirational quotes for each row in the dataset. It loops through the rows, uses the seed text (the text from the selected column), and generates quotes using the fine-tuned GPT-2 model. The generated quotes are stored along with their source row index.\n\n7. **Create a DataFrame for Generated Quotes:**\n   - The generated quotes are organized into a new DataFrame. Each row in the DataFrame contains a generated quote and the source row it was based on.\n\n8. **Save Generated Quotes:**\n   - Finally, the code saves the generated quotes to a new CSV file called \"generated_quotes.csv.\"\n","metadata":{}},{"cell_type":"code","source":"# Use a pipeline as a high-level helper\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"noelmathewisaac/inspirational-quotes-distilgpt2\")","metadata":{"execution":{"iopub.status.busy":"2023-10-23T23:00:09.791477Z","iopub.execute_input":"2023-10-23T23:00:09.792343Z","iopub.status.idle":"2023-10-23T23:00:12.216355Z","shell.execute_reply.started":"2023-10-23T23:00:09.792309Z","shell.execute_reply":"2023-10-23T23:00:12.215572Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"noelmathewisaac/inspirational-quotes-distilgpt2\")\nmodel = AutoModelForCausalLM.from_pretrained(\"noelmathewisaac/inspirational-quotes-distilgpt2\")","metadata":{"execution":{"iopub.status.busy":"2023-10-23T23:00:16.886579Z","iopub.execute_input":"2023-10-23T23:00:16.886989Z","iopub.status.idle":"2023-10-23T23:00:19.464560Z","shell.execute_reply.started":"2023-10-23T23:00:16.886959Z","shell.execute_reply":"2023-10-23T23:00:19.463746Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import random\nimport torch\nimport pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Check if a GPU is available and use it\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Load the tokenizer and model on the specified device\ntokenizer = AutoTokenizer.from_pretrained(\"noelmathewisaac/inspirational-quotes-distilgpt2\")\nmodel = AutoModelForCausalLM.from_pretrained(\"noelmathewisaac/inspirational-quotes-distilgpt2\").to(device)\n\n# Load the preprocessed dataset\npreprocessed_data = pd.read_csv('/kaggle/input/pre-pro-quio/preprocessed_quotes_no_quote_author_category.csv')  # Replace with your dataset path\n\n# Specify the column containing text data\ntext_column = \"first_3_words\"  # Adjust this to the actual column name containing text data\n\n# Create an empty list to store generated quotes\ngenerated_quotes = []\n\n\nmax_length = 100\n\n\ndef generate_quotes_for_each_row(dataframe, max_rows):\n    for index, row in dataframe.iterrows():\n        if index >= max_rows:\n            break\n        entry = row[text_column]\n\n        seed_text = entry\n        input_ids = tokenizer.encode(seed_text, return_tensors=\"pt\").to(device)\n        attention_mask = torch.ones(input_ids.shape, device=device)\n        output = model.generate(input_ids, max_length=max_length, no_repeat_ngram_size=20, top_k=50, pad_token_id=model.config.eos_token_id, attention_mask=attention_mask)\n        quote = tokenizer.decode(output[0], skip_special_tokens=True)\n\n        generated_quotes.append({\"Generated_Quote\": quote, \"Source_Row\": f\"Row {index + 1}\"})\n\n# Generate up to 1 quote per row for the maximum number of rows\ngenerate_quotes_for_each_row(preprocessed_data, max_rows=len(preprocessed_data))\n\n# Create a DataFrame from the generated quotes\ngenerated_quotes_data = pd.DataFrame(generated_quotes)\n\n# Save the generated quotes to a new CSV file\ngenerated_quotes_data.to_csv('generated_quotes.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-23T23:00:24.290117Z","iopub.execute_input":"2023-10-23T23:00:24.291071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**vggh","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}